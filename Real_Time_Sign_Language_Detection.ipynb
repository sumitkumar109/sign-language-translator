{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Introduction"
      ],
      "metadata": {
        "id": "-qXqOXjQ7ruy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Real time sign language detector"
      ],
      "metadata": {
        "id": "3QqS8hZt7u3I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Sign Language Recognition is a breakthrough for helping deaf-mute people and has been researched for many years.  The methods used in developing Sign Language Recognition are also varied between researchers. The aim of this project is to review the sign language recognition approaches and find the best method that can be used to create and deploy a real life sign language detector.\n",
        "##### Research source: [Link](https://www.researchgate.net/publication/357622360_Real_Time_Sign_Language_Detection)"
      ],
      "metadata": {
        "id": "qBjqd5Mc719Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Machine Learning framework and model selection"
      ],
      "metadata": {
        "id": "P528VWX379TM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### The project is about detecting sign language hand gestures, so the dataset used was a custom dataset. The required model should use Convolutional Neural Network (CNN) for training and it should be flexible enough so that it can train on custom images. An open-source library that satisfies the above mentioned criteria is Ultralytics YOLOv5 build in collaboration with facebook's PyTorch framework.\n",
        "<img src=\"https://user-images.githubusercontent.com/26456083/86477109-5a7ca780-bd7a-11ea-9cb7-48d9fd6848e7.jpg\"\n",
        "     alt=\"Markdown Monster icon\"\n",
        "     style=\"float: left; margin-right: 10px; width: 300px; height: 150px;\" />"
      ],
      "metadata": {
        "id": "esD9JxIP7__K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Research paper on YOLO technology by it's developer Alexey Bochkovskiy: [Link](https://arxiv.org/abs/2004.10934)\n",
        "##### A guide to implement YOLOv5 framework: [Link](https://github.com/ultralytics/yolov5)"
      ],
      "metadata": {
        "id": "w350eO-m8DKU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### PyTorch YOLOv5 model analysis"
      ],
      "metadata": {
        "id": "0ulHdRxs8TjM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### YOLO model is a fast compact object detection model that is very performant relative to its size. YOLO model is a fast compact object detection model that is very performant relative to its size and it has been steadily improving. The YOLO network consists of three main pieces:\n",
        "* **Backbone**\n",
        "* **Neck**\n",
        "* **Head**\n",
        "\n",
        "<img src=\"https://blog.roboflow.com/content/images/2020/06/image-10.png\"\n",
        "     alt=\"Markdown Monster icon\"\n",
        "     style=\"float: left; margin-right: 10px; width: 600px; height: 200px;\" />"
      ],
      "metadata": {
        "id": "E7ghVOHC8YS7"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jgO0BoaE7iuj"
      },
      "source": [
        "# 2. Install and Import Dependencies"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x4tYpo7o7iuw"
      },
      "source": [
        "### Install PyTorch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d57TZPsq7iuy"
      },
      "outputs": [],
      "source": [
        "# !pip install torch==1.8.2+cpu torchvision==0.9.2+cpu torchaudio===0.8.2 -f https://download.pytorch.org/whl/lts/1.8/torch_lts.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n2d9T6GH7iu1"
      },
      "source": [
        "### Clone and Install the YOLOv5 library"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MQKS8_-Z7iu3"
      },
      "outputs": [],
      "source": [
        "# !git clone https://github.com/ultralytics/yolov5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jMi9fR8m7iu4"
      },
      "outputs": [],
      "source": [
        "# cd yolov5 "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8jaYJJpq7iu6"
      },
      "outputs": [],
      "source": [
        "# !pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XI-OcfPR7iu7"
      },
      "source": [
        "### Load the initial model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jwA4MIpU7iu9"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "import cv2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ilGfpvh47iu_"
      },
      "outputs": [],
      "source": [
        "# model = torch.hub.load('ultralytics/yolov5', 'yolov5s')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7sKUHonn7ivB"
      },
      "source": [
        "##### Copy the yolov5s.pt weights file inside the yolov5 directory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PbxLS0uw7ivC"
      },
      "source": [
        "# 3. Train from scratch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZONG5avd7ivD"
      },
      "outputs": [],
      "source": [
        "import uuid\n",
        "import os\n",
        "import time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2fgMAViP7ivE"
      },
      "source": [
        "### Collect images for different sign language poses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NkMPGbZy7ivF"
      },
      "outputs": [],
      "source": [
        "# IMAGES_PATH = os.path.join('data', 'images')\n",
        "# labels = ['hello', 'thanks', 'yes', 'no', 'iloveyou']\n",
        "# number_imgs = 15"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0K8vMbhi7ivF",
        "outputId": "acd19070-faf1-4a3a-f04d-9ce246c82e49"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"\\ncap = cv2.VideoCapture(0)\\n# Loop through labels\\nfor label in labels:\\n    print('Collecting images for {}'.format(label))\\n    time.sleep(5)\\n    \\n    # Loop through image range\\n    for img_num in range(number_imgs):\\n        print('Collecting images for {}, image number {}'.format(label, img_num))\\n        \\n        # Webcam feed\\n        ret, frame = cap.read()\\n        \\n        # Naming out image path\\n        imgname = os.path.join(IMAGES_PATH, label+'.'+str(uuid.uuid1())+'.jpg')\\n        \\n        # Writes out image to file \\n        cv2.imwrite(imgname, frame)\\n        \\n        # Render to the screen\\n        cv2.imshow('Image Collection', frame)\\n        \\n        # 2 second delay between captures\\n        time.sleep(2)\\n        \\n        if cv2.waitKey(10) & 0xFF == ord('q'):\\n            break\\ncap.release()\\ncv2.destroyAllWindows()\\n\""
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "'''\n",
        "cap = cv2.VideoCapture(0)\n",
        "# Loop through labels\n",
        "for label in labels:\n",
        "    print('Collecting images for {}'.format(label))\n",
        "    time.sleep(5)\n",
        "    \n",
        "    # Loop through image range\n",
        "    for img_num in range(number_imgs):\n",
        "        print('Collecting images for {}, image number {}'.format(label, img_num))\n",
        "        \n",
        "        # Webcam feed\n",
        "        ret, frame = cap.read()\n",
        "        \n",
        "        # Naming out image path\n",
        "        imgname = os.path.join(IMAGES_PATH, label+'.'+str(uuid.uuid1())+'.jpg')\n",
        "        \n",
        "        # Writes out image to file \n",
        "        cv2.imwrite(imgname, frame)\n",
        "        \n",
        "        # Render to the screen\n",
        "        cv2.imshow('Image Collection', frame)\n",
        "        \n",
        "        # 2 second delay between captures\n",
        "        time.sleep(2)\n",
        "        \n",
        "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
        "            break\n",
        "cap.release()\n",
        "cv2.destroyAllWindows()\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZNjU_NPp7ivJ"
      },
      "outputs": [],
      "source": [
        "# print(os.path.join(IMAGES_PATH, labels[0]+'.'+str(uuid.uuid1())+'.jpg'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ygesjwnN7ivK",
        "outputId": "2fe053e4-1d88-4e33-ec3c-72f5e938e845"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"\\nfor label in labels:\\n    print('Collecting images for {}'.format(label))\\n    for img_num in range(number_imgs):\\n        print('Collecting images for {}, image number {}'.format(label, img_num))\\n        imgname = os.path.join(IMAGES_PATH, label+'.'+str(uuid.uuid1())+'.jpg')\\n        print(imgname)  \\n\""
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "'''\n",
        "for label in labels:\n",
        "    print('Collecting images for {}'.format(label))\n",
        "    for img_num in range(number_imgs):\n",
        "        print('Collecting images for {}, image number {}'.format(label, img_num))\n",
        "        imgname = os.path.join(IMAGES_PATH, label+'.'+str(uuid.uuid1())+'.jpg')\n",
        "        print(imgname)  \n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "loM69zSf7ivL"
      },
      "source": [
        "### Clone and Install the labelImg library for labeling the images in YOLO format\n",
        "##### The collected and labelled images will then be used in Google Colab GPU for training purposes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FQudvGy_7ivM"
      },
      "outputs": [],
      "source": [
        "# !git clone https://github.com/tzutalin/labelImg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "yCn2_G257ivM"
      },
      "outputs": [],
      "source": [
        "# !pip install pyqt5 lxml --upgrade\n",
        "# !cd labelImg && pyrcc5 -o libs/resources.py resources.qrc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "69W-uBZ17ivN"
      },
      "source": [
        "##### Next from inside the labelImg directory open the command prompt and run: python labelImg.py\n",
        "##### This will start the labelImg GUI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wQhijDQb7ivN"
      },
      "source": [
        "# 4. Load Custom Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hCOwEXWQ7ivO"
      },
      "source": [
        "### After training the model on Google Colab GPU use the generated last weight file present inside the latest experiment directory to load the custom model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": false,
        "id": "yDXlF7zD7ivP",
        "outputId": "0726905b-f837-40fc-c049-31f1b782fb9e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading: \"https://github.com/ultralytics/yolov5/archive/master.zip\" to C:\\Users\\Administrator/.cache\\torch\\hub\\master.zip\n",
            "YOLOv5  2022-4-3 torch 1.8.2+cpu CPU\n",
            "\n",
            "Fusing layers... \n",
            "Model summary: 213 layers, 7064065 parameters, 0 gradients, 16.0 GFLOPs\n",
            "Adding AutoShape... \n"
          ]
        }
      ],
      "source": [
        "model = torch.hub.load('ultralytics/yolov5', 'custom', path='yolov5/runs/train/exp5/weights/last.pt', force_reload=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5t-f3eis7ivP"
      },
      "outputs": [],
      "source": [
        "model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.conf = 0.75"
      ],
      "metadata": {
        "id": "BmWVPArmuSLD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "smt5qr7G7ivQ"
      },
      "source": [
        "# 5. Making Predictions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r0uvy_vA7ivQ"
      },
      "source": [
        "### Making prediction on a single image file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Kves4hy7ivR"
      },
      "outputs": [],
      "source": [
        "img = os.path.join('data', 'images', 'hello.0a5a5a9c-ab7a-11ec-a3c3-7470fdfa1dff.jpg')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RDaVJkbB7ivR"
      },
      "outputs": [],
      "source": [
        "results = model(img)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kjs7TrmH7ivS",
        "outputId": "fb8230ea-4e8d-452a-91a0-60cf9926c2a7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "image 1/1: 480x640 1 hello\n",
            "Speed: 25.1ms pre-process, 459.1ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n"
          ]
        }
      ],
      "source": [
        "results.print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UgPM0Jwo7ivS"
      },
      "outputs": [],
      "source": [
        "results.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XV3FJv2a7ivT"
      },
      "source": [
        "### Real Time Sign Language Detection using webcam"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oqwoom-g7ivT"
      },
      "outputs": [],
      "source": [
        "cap = cv2.VideoCapture(0)\n",
        "while cap.isOpened():\n",
        "    ret, frame = cap.read()\n",
        "    \n",
        "    # Make detections \n",
        "    results = model(frame)\n",
        "    \n",
        "    cv2.imshow('YOLO', np.squeeze(results.render()))\n",
        "    \n",
        "    if cv2.waitKey(10) & 0xFF == ord('q'):\n",
        "        break\n",
        "cap.release()\n",
        "cv2.destroyAllWindows()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}